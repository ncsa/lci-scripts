# Lab: Configuration Management Tools - Commands
# Replace XX with your cluster number (e.g., 01, 02, etc.)
# Using vim - vim commands, :%s/XX/<clusternumber>/g, :wq
# Using nano - nano commands, Ctrl+\, Enter "XX", Enter your cluster number, A, Ctrl+O, Enter, Ctrl+X

# Run as root
sudo -i
cd ~

# Clone repository
git clone https://github.com/ncsa/lci-scripts.git

# Copy playbook and configure
cp -a lci-scripts/introductory/2026/config_mgmt/Config_mgmt_playbook .
cd Config_mgmt_playbook
# edit hosts.ini

# ============================================================
# Part 1: Ansible Recap
# ============================================================

# Test connectivity
ansible all_nodes -i hosts.ini -m ping

# Ad-hoc commands - gather info
ansible all_nodes -i hosts.ini -a "uptime"
ansible all_nodes -i hosts.ini -a "df -h"
ansible all_nodes -i hosts.ini -a "hostname"
ansible all_nodes -i hosts.ini -m shell -a "cat /etc/os-release | head -2"

# Ad-hoc commands - use modules
ansible all_nodes -i hosts.ini -m setup -a "filter=ansible_memory_mb"
ansible compute -i hosts.ini -m dnf -a "name=htop state=present" --become
ansible compute -i hosts.ini -m dnf -a "name=tmux state=present" --become

# Verify packages installed
ansible compute -i hosts.ini -m shell -a "rpm -q htop tmux"

# Copy a file to all nodes
ansible all_nodes -i hosts.ini -m copy -a "content='Hello from LCI 2026\n' dest=/tmp/lci_test.txt" --become
ansible all_nodes -i hosts.ini -a "cat /tmp/lci_test.txt"

# Create user playbook called "create_user.yml" with the following content:
---
- name: Create workshop user on all nodes
  hosts: all_nodes
  become: yes
  tasks:
    - name: Create workshop user
      user:
        name: workshop
        uid: 3000
        state: present
        comment: "Config Management Workshop User"
    - name: Ensure .ssh directory exists
      file:
        path: /home/workshop/.ssh
        state: directory
        owner: workshop
        group: workshop
        mode: '0700'



ansible-playbook -i hosts.ini create_user.yml
ansible all_nodes -i hosts.ini -a "id workshop"

# Run the included playbook.yml to install Salt via Ansible (used in Part 2)
ansible-playbook -i hosts.ini playbook.yml

# ============================================================
# Part 2: Salt
# ============================================================

# Install Salt Master
dnf install -y salt-master

cat > /etc/salt/master.d/lci.conf << 'EOF'
auto_accept: True
file_roots:
  base:
    - /srv/salt
pillar_roots:
  base:
    - /srv/pillar
EOF

mkdir -p /srv/salt /srv/pillar
systemctl enable --now salt-master

# Install Salt Minion (head node)
dnf install -y salt-minion

# Using your preferred editor (vim or nano), create /etc/salt/minion.d/master.conf
# The file should contain the following two lines:
#   master: lci-head-XX-1
#   id: lci-head-XX-1
vim /etc/salt/minion.d/master.conf

systemctl enable --now salt-minion

# Install Salt Minion (compute nodes)
clush -g compute "dnf install -y salt-minion"
clush -g compute "echo 'master: lci-head-01-1' > /etc/salt/minion.d/master.conf"
clush -g compute 'echo "id: ${HOSTNAME}" > /etc/salt/minion.d/id.conf --host ${HOSTNAME}'
clush -g compute "systemctl enable --now salt-minion"
clush -g compute "systemctl restart salt-minion"

# Verify minions
salt-key -L
salt '*' test.ping

# Create Salt state for user
cat > /srv/salt/create_user.sls << 'EOF'
workshop_user:
  user.present:
    - name: workshop_salt
    - uid: 3001
    - fullname: Salt Workshop User

workshop_ssh_dir:
  file.directory:
    - name: /home/workshop_salt/.ssh
    - user: workshop_salt
    - group: workshop_salt
    - mode: 700
    - require:
      - user: workshop_user
EOF

salt '*' state.apply create_user
salt '*' cmd.run "id workshop_salt"

# Salt grains
salt '*' grains.items
salt '*' grains.get os
salt -G 'os:Rocky' test.ping

# Salt ad-hoc commands
salt '*' cmd.run "uptime"
salt '*' pkg.install htop
salt '*' disk.usage

# ============================================================
# Part 3: Combined Exercises - Using Ansible and Salt Together
# ============================================================

# --- Exercise 1: Ansible installs packages, Salt configures them ---
# Use Ansible to install a package across all nodes
ansible all_nodes -i hosts.ini -m dnf -a "name=tree state=present" --become

# Use Salt to verify the install and gather info
salt '*' pkg.version tree
salt '*' cmd.run "tree --version"

# --- Exercise 2: Ansible playbook deploys config, Salt enforces state ---
# Create a playbook that sets up an MOTD on all nodes
cat > deploy_motd.yml << 'EOF'
---
- name: Deploy MOTD to all nodes
  hosts: all_nodes
  become: yes
  tasks:
    - name: Set MOTD
      copy:
        content: |
          ==========================================
          Welcome to {{ inventory_hostname }}
          LCI 2026 Workshop Cluster
          Managed by Ansible and Salt
          ==========================================
        dest: /etc/motd

    - name: Set login banner
      copy:
        content: "Authorized users only.\n"
        dest: /etc/issue
EOF

ansible-playbook -i hosts.ini deploy_motd.yml

# Verify with Salt
salt '*' cmd.run "cat /etc/motd"

# Use Salt to create a state that enforces the MOTD content going forward
cat > /srv/salt/motd.sls << 'EOF'
motd_file:
  file.managed:
    - name: /etc/motd
    - contents: |
        ==========================================
        This system is managed by Salt
        LCI 2026 Workshop Cluster
        ==========================================
EOF

salt '*' state.apply motd

# Notice Salt overwrote the Ansible-deployed MOTD - this demonstrates
# why choosing one tool as the source of truth matters!
salt '*' cmd.run "cat /etc/motd"

# --- Exercise 3: Ansible orchestrates, Salt manages state ---
# Use Ansible to trigger Salt state runs across targeted nodes
cat > salt_orchestrate.yml << 'EOF'
---
- name: Use Ansible to orchestrate Salt
  hosts: head
  become: yes
  tasks:
    - name: Sync Salt modules
      command: salt '*' saltutil.sync_all

    - name: Apply user state on all minions
      command: salt '*' state.apply create_user

    - name: Check node health via Salt grains
      command: salt '*' grains.get os
      register: os_info

    - name: Display OS info
      debug:
        var: os_info.stdout_lines

    - name: Install packages via Salt on compute nodes only
      command: salt -G 'id:lci-compute-*' pkg.install vim-enhanced
EOF

ansible-playbook -i hosts.ini salt_orchestrate.yml

# ============================================================
# Reference: Warewulf (requires bare-metal or properly networked cluster)
# ============================================================

## Install Warewulf
#ansible-playbook -i hosts.ini warewulf.yml
#
## Basic wwctl commands
#wwctl version
#wwctl --help
#
## Working with Images
#wwctl image import docker://ghcr.io/warewulf/warewulf-rockylinux:9 rockylinux-9
#wwctl image list
#wwctl image list --long
#
## Modify image interactively
#wwctl image shell rockylinux-9
#dnf -y install htop vim
#exit
#
## Execute command in image
#wwctl image exec rockylinux-9 -- /usr/bin/dnf -y install tmux
#
## Build image manually
#wwctl image build rockylinux-9
#
## Show image details
#wwctl image show rockylinux-9
#
## Working with Overlays
#wwctl overlay list
#wwctl overlay list --all wwinit
#
## Create custom overlay
#wwctl overlay create site-custom
#wwctl overlay import site-custom /etc/motd --parents
#
## Edit overlay file (add template content)
#cat > /var/lib/warewulf/overlays/site-custom/rootfs/etc/motd << 'EOF'
#Welcome to {{.Id}}
#This node is part of the LCI 2026 workshop cluster.
#Managed by Warewulf v4
#EOF
#
#wwctl overlay show site-custom /etc/motd
#wwctl overlay build
#
## Working with Nodes
#wwctl node add lci-compute-XX-1 --ipaddr=10.0.1XX.1 --discoverable=true --image=rockylinux-9
#wwctl node add lci-compute-XX-2 --ipaddr=10.0.1XX.2 --discoverable=true --image=rockylinux-9
#
#wwctl node list
#wwctl node list --all
#
## Set node properties
#wwctl node set lci-compute-XX-1 --image=rockylinux-9
#wwctl node set lci-compute-XX-1 \
#  --system-overlays="wwinit,wwclient,fstab,hostname,ssh.host_keys,site-custom"
#
## Configure services
#wwctl configure dhcp
#wwctl configure nfs
#
## Working with Profiles
#wwctl profile list
#wwctl profile add lci-compute --image=rockylinux-9
