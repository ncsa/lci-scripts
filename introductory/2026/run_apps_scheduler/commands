# Lab: Run Application via Scheduler - Commands
# Replace XX with your cluster number (e.g., 01, 02, etc.)
# Using vim - vim commands, :%s/01/<clusternumber>/g, :wq
# Using nano - nano commands, Ctrl+\, Enter "01", Enter your cluster number, A, Ctrl+O, Enter, Ctrl+X

# Run as root
sudo -i
cd

# If you are reading this in a browser - Clone repository to your lci-head-XX-1
git clone https://github.com/ncsa/lci-scripts.git

# Copy the scheduler playbook to your home directory and work from there
cd ~
cp -a lci-scripts/introductory/2026/run_apps_scheduler/Run_apps_scheduler .
cd Run_apps_scheduler

# 1. Create Linux user mpiuser on all nodes (must exist everywhere for Slurm jobs)
useradd -u 2004 mpiuser
clush -g compute "useradd -u 2004 mpiuser"

# Copy the lab source code to /scratch/ (shared filesystem visible from all nodes)
mkdir -p /scratch/mpiuser
cp -r MPI OpenMP simple /scratch/mpiuser/
chown -R mpiuser:mpiuser /scratch/mpiuser

# Verify files are in place
ls -la /scratch/mpiuser/OpenMP/
ls -la /scratch/mpiuser/MPI/

# Create Slurm accounts and users
sacctmgr -i add account lci2026 Description="LCI 2026 workshop"
sacctmgr -i create user name=mpiuser cluster=cluster account=lci2026
sacctmgr -i create user name=rocky cluster=cluster account=lci2026
sacctmgr list associations format=user,account

# 2. Check cluster resources
sinfo
sinfo -N -l
squeue

# 3. salloc - allocate resources for OpenMP run
sudo su - mpiuser
cd /scratch/mpiuser/OpenMP
salloc -n 2

# Check allocated resources
sinfo
squeue
sinfo -N -l

# Compile and run OpenMP app
gcc -fopenmp -o heated_plate.x heated_plate_openmp.c -lm
export OMP_NUM_THREADS=2
./heated_plate.x

# Exit salloc, check resources and accounting
exit
sinfo
sacct -u mpiuser

# 4. srun - interactive runs

# OpenMP interactive run
srun -n 2 --pty bash
cd /scratch/mpiuser/OpenMP
export OMP_NUM_THREADS=2
./heated_plate.x
exit
sacct -u mpiuser

# Check MPI support
srun --mpi=list

# Set up OpenMPI environment (required for mpicc and MPI runs)
export PATH=/opt/openmpi/5.0.1/bin:$PATH
export LD_LIBRARY_PATH=/opt/openmpi/5.0.1/lib:$LD_LIBRARY_PATH

# Remove the default stack size limit to prevent segfaults from large stack-allocated
# arrays (e.g. mpi_heat2D.c allocates ~8MB on the stack). This must be set before
# launching MPI jobs interactively. Batch scripts include this automatically.
ulimit -s unlimited

# Compile MPI applications
cd /scratch/mpiuser/MPI
mpicc -o mpi_heat2D.x mpi_heat2D.c -lm
mpicc -o poisson_mpi.x poisson_mpi.c

# MPI interactive runs
srun --mpi=pmix -n 4 mpi_heat2D.x
srun --mpi=pmix_v4 -n 4 mpi_heat2D.x
sacct -u mpiuser

# 5. sbatch - submit batch scripts

# MPI batch (uses srun --mpi=pmix, NOT mpirun)
cd /scratch/mpiuser/MPI
cat mpi_batch.sh
sbatch mpi_batch.sh
squeue
# Check output: cat slurm-<jobid>.out

# Also available: mpi_srun.sh (identical, both use srun)
sbatch mpi_srun.sh

# OpenMP batch
cd /scratch/mpiuser/OpenMP
cat openmp_batch.sh
sbatch openmp_batch.sh
# Check output: cat run.out-<jobid> and cat slurm.out

# 6. scancel - terminate a job
sbatch -w lci-compute-XX-1 openmp_batch.sh
squeue
scancel -j <jobid>

# 7. Concurrent jobs - observe resource contention
cd /scratch/mpiuser/MPI
srun -n 2 --pty bash
# In another terminal, submit a job that will pend:
sbatch mpi_batch.sh
squeue
# Job shows NODES=2 with PD (Resources) - waiting for the 2 CPUs from your srun
# Exit srun to free resources, then the pending job should start
exit
squeue

# 8. sstat - monitor running jobs

cd /scratch/mpiuser/MPI
sbatch poisson_batch.sh

# Monitor the job
sstat -j <jobid>
sstat -p --format=AveCPU,AvePages,AveRSS,AveVMSize,JobID -j <jobid>

# 9. sacct - job accounting
sacct --helpformat
sacct -j <jobid> --format="JobID,JobName,State,Start,End"
sacct -j <jobid> --format="JobID,JobName%15,State,NodeList%20,CPUTime"

# 10. scontrol - cluster and job info
scontrol show config
scontrol show node lci-compute-XX-1
scontrol show job <jobid>

# Submit another job and check its info
cd /scratch/mpiuser/OpenMP
sbatch openmp_batch.sh
scontrol show job <jobid>

# 11. Simple bash example - submitting a basic script via sbatch
# This demonstrates sbatch with a plain bash script (no MPI/OpenMP needed)

cd /scratch/mpiuser/simple
cat count.sh
cat count_batch.sh

# Submit the counting job
sbatch count_batch.sh
squeue

# After the job completes, check the output
cat slurm_count.out
sacct -u mpiuser
